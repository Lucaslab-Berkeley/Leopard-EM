{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae37b7b9",
   "metadata": {},
   "source": [
    "# Constrained Search tutorial\n",
    "\n",
    "This is a complete tutorial going through the process of locating the 40S small ribosomal subunit using the constrained orientation search from identified 60S large ribosomal subunits.\n",
    "We step through the process generating the reference template maps, running the template matching program, optimizing the template, and finally running the constrained orientation search.\n",
    "Input data for this tutorial as well as the intermediary result files can be found at [this Zenodo dataset 10.5281/zenodo.15368246](https://zenodo.org/records/15368246).\n",
    "We will be using the micrograph with filename `xenon_131_000_0.0_DWS.mrc`, but this process can be done for a whole cohort of images.\n",
    "\n",
    "*Note: Some of the file links in the text are relative paths and may not work for the online documentation. Downloading the notebook and required data locally will fix this, or just inspect the files directly on Zenodo.*\n",
    "\n",
    "### Tutorial Requirements\n",
    "\n",
    "In terms of Python libraries, the following are required\n",
    "\n",
    "* Leopard-EM v1.0 or above\n",
    "* matplotlib\n",
    "* TODO\n",
    "\n",
    "You will also need a CUDA capable GPU for running some of the analyses.\n",
    "However, you can alternately download the intermediary results from Zenodo instead of running the GPU programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47483415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell to install required packages\n",
    "# !pip install leopard-em matplotlib\n",
    "# TODO: test this and verify which packages are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mmdf\n",
    "import mrcfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import roma\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "from scipy.spatial.transform import Rotation\n",
    "from ttsim3d.models import Simulator, SimulatorConfig\n",
    "\n",
    "from leopard_em.analysis.zscore_metric import gaussian_noise_zscore_cutoff\n",
    "from leopard_em.pydantic_models.managers import (\n",
    "    ConstrainedSearchManager,\n",
    "    MatchTemplateManager,\n",
    "    OptimizeTemplateManager,\n",
    "    RefineTemplateManager,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b2192",
   "metadata": {},
   "source": [
    "## 1. Download and pre-process required data\n",
    "\n",
    "The following cells will go through, download and pre-process all the necessary data to process in this tutorial.\n",
    "This will also create a directory structure to save the micrographs, models, maps, and configuration files.\n",
    "\n",
    "We also include a few visualizations to see what data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ed73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_zenodo_file(url: str, out_dir: str) -> str:\n",
    "    \"\"\"Helper function to download a file hosted on Zenodo from a URL to given dir.\"\"\"\n",
    "    output_filename = url.split(\"/\")[-1]\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "\n",
    "    with open(f\"{out_dir}/{output_filename}\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "file_downloads = [\n",
    "    (\"mgraphs\", \"https://zenodo.org/records/15368246/files/xenon_131_000_0.0_DWS.mrc\"),\n",
    "    (\"models\",  \"https://zenodo.org/records/15368246/files/60S_aligned.pdb\"),\n",
    "    (\"models\",  \"https://zenodo.org/records/15368246/files/6q8y_aligned.pdb\"),\n",
    "    (\"models\",  \"https://zenodo.org/records/15368246/files/6q8y_SSU_no_head_aligned.pdb\"),\n",
    "    (\"models\",  \"https://zenodo.org/records/15368246/files/3j77_SSU_aligned_zero.pdb\"),\n",
    "    (\"models\",  \"https://zenodo.org/records/15368246/files/3j78_SSU_aligned_zero.pdb\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/match_template_config_crop.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/optimize_template_config.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/match_template_config_60S.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/match_template_config_40S.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/refine_template_config_60S.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/constrained_config_step1.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/constrained_config_step2.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/constrained_config_step3.yaml\"),\n",
    "    (\"configs\", \"https://zenodo.org/records/15368246/files/constrained_config_step4.yaml\"),\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# Loop through files list and download each\n",
    "for out_dir, file_url in file_downloads:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Skip if the file already exists\n",
    "    fname = file_url.split(\"/\")[-1]\n",
    "    if os.path.exists(f\"{out_dir}/{fname}\"):\n",
    "        print(f\"Skipped {fname}, it already exists in {out_dir}.\")\n",
    "        continue\n",
    "\n",
    "    # Download the file\n",
    "    filename = download_zenodo_file(file_url, out_dir)\n",
    "    print(f\"Downloaded {filename} to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8615d",
   "metadata": {},
   "source": [
    "### Plot of the micrograph\n",
    "\n",
    "Here, we use the `mrcfile` package to read a .mrc file into a numpy array.\n",
    "Then, we visualize the micrograph with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b70a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image into numpy array called 'data'\n",
    "data = mrcfile.open(\"mgraphs/xenon_131_000_0.0_DWS.mrc\", mode=\"r\").data.copy()\n",
    "\n",
    "# Plot the greyscale image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f477de",
   "metadata": {},
   "source": [
    "### Pre-process PDB files\n",
    "\n",
    "We've downloaded a PDB model of the 80S ribosome in the non-rotated state `6q8y_aligned.pdb`.\n",
    "In addition to this, we have two additional PDB files which correspond to the 40S (`6q8y_SSU_no_head_aligned.pdb`) and 60S (`60S_aligned.pdb`) ribosomal subunits; these were generated externally using the ChimeraX program from the full non-rotated ribosome model.\n",
    "For the 40S subunit, the head domain has also been removed to leave only the body.\n",
    "Both the 40S and 60S models have been pre-aligned with respect to the 80S model suing the matchmaker function in ChimeraX so the relative positions and orientations of the models match with each other.\n",
    "\n",
    "#### Center PDB models\n",
    "\n",
    "The 80S PDB file is shifted such that the average atomic position is located at $(0, 0, 0)$.\n",
    "This same shift is applied to the 40S and 60S models so they remain aligned throughout, and all transformed PDB files are written back to disk with an `_aligned_zero` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f444f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_pdb_files(pdb_ref: str, pdb_A: str, pdb_B: str) -> None:\n",
    "    \"\"\"Transform reference PDB file to average atomic position of (0, 0, 0).\n",
    "\n",
    "    The same transformation is applied to the other PDB files, A and B, and all files\n",
    "    are saved with a new '_aligned_zero' suffix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_ref : str\n",
    "        Path to reference PDB file to center.\n",
    "    pdb_A : str\n",
    "        Additional PDB file to also transform based on reference centering.\n",
    "    pdb_B : str\n",
    "        Additional PDB file to also transform based on reference centering.\n",
    "    \"\"\"\n",
    "    # Load PDB models into DataFrame objects\n",
    "    df_ref = mmdf.read(pdb_ref)\n",
    "    df_A = mmdf.read(pdb_A)\n",
    "    df_B = mmdf.read(pdb_B)\n",
    "\n",
    "    # Extract atom coordinates from reference PDB. Shape of (n_atoms, 3)\n",
    "    coords = df_ref[[\"x\", \"y\", \"z\"]].values\n",
    "    center = np.mean(coords, axis=0)\n",
    "\n",
    "    print(f\"Center of reference PDB: {center}\")\n",
    "\n",
    "    # Now apply the centering transformation to PDB files\n",
    "    shift_vector = -center\n",
    "    df_ref[[\"x\", \"y\", \"z\"]] += shift_vector\n",
    "    df_A[[\"x\", \"y\", \"z\"]] += shift_vector\n",
    "    df_B[[\"x\", \"y\", \"z\"]] += shift_vector\n",
    "\n",
    "    # Save the transformed PDB files with a new name\n",
    "    mmdf.write(pdb_ref.replace(\".pdb\", \"_aligned_zero.pdb\"), df_ref)\n",
    "    mmdf.write(pdb_A.replace(\".pdb\", \"_aligned_zero.pdb\"), df_A)\n",
    "    mmdf.write(pdb_B.replace(\".pdb\", \"_aligned_zero.pdb\"), df_B)\n",
    "\n",
    "\n",
    "# Center the PDB files\n",
    "center_pdb_files(\n",
    "    pdb_ref=\"models/60S_aligned.pdb\",\n",
    "    pdb_A=\"models/6q8y_aligned.pdb\",\n",
    "    pdb_B=\"models/6q8y_SSU_no_head_aligned.pdb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca751fc2",
   "metadata": {},
   "source": [
    "## 2. Initial match template with 60S model\n",
    "\n",
    "Since we want to constrain the search space for a 40S small subunit (SSU) using the 60S large subunit (LSU), we first need to run full-orientation match template on the LSU model.\n",
    "We will go through the steps of configuring and running the match template program in Python.\n",
    "Further details about the match template program in Leopard-EM are located [here on the documentation](TODO-link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023f391",
   "metadata": {},
   "source": [
    "### Generating 3D maps from models\n",
    "\n",
    "The template matching program requires simulated 3D maps to generate projections form, and below we use the [ttsim3d](https://github.com/teamtomo/ttsim3d) Python package to generate these maps.\n",
    "For a different dataset/structure, these simulation configurations need to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a directory to save 3D map files\n",
    "os.makedirs(\"maps\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cace0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the simulation configuration object\n",
    "sim_conf = SimulatorConfig(\n",
    "    voltage=300.0,  # in keV\n",
    "    apply_dose_weighting=True,\n",
    "    dose_start=0.0,  # in e-/A^2\n",
    "    dose_end=50.0,  # in e-/A^2\n",
    "    dose_filter_modify_signal=\"rel_diff\",\n",
    "    upsampling=-1,  # auto\n",
    "    mtf_reference=\"falcon4EC_300kv\",\n",
    ")\n",
    "\n",
    "# Instantiate the simulator\n",
    "sim = Simulator(\n",
    "    pdb_filepath=\"models/60S_aligned_aligned_zero.pdb\",\n",
    "    pixel_spacing=0.95,  # Angstroms\n",
    "    volume_shape=(512, 512, 512),\n",
    "    center_atoms=False,\n",
    "    remove_hydrogens=True,\n",
    "    b_factor_scaling=0.5,  # Multiply model b-factors by 1/2\n",
    "    additional_b_factor=0,\n",
    "    simulator_config=sim_conf,\n",
    ")\n",
    "\n",
    "# Run the simulation and write the output to a file\n",
    "# We will read this file into memory later\n",
    "mrc_filepath = \"maps/60S_map_px0.95_bscale0.5.mrc\"\n",
    "sim.export_to_mrc(mrc_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7b6c4",
   "metadata": {},
   "source": [
    "### Plotting a z-slice of the simulated map\n",
    "\n",
    "Just for visualization purposes, we plot the central z-slice of the simulated 60S map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = mrcfile.open(mrc_filepath, mode=\"r\").data.copy()\n",
    "\n",
    "plt.imshow(volume[256, :, :], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba30c6",
   "metadata": {},
   "source": [
    "### Initial test template matching run\n",
    "\n",
    "Below, we run the match template program on a small image patch which will give us a few peaks and optimize our template simulation before proceeding.\n",
    "First, we crop our a central 1k by 1k patch from our 4k by 4k image and save it as a new mrc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mrcfile.open(\"mgraphs/xenon_131_000_0.0_DWS.mrc\", mode=\"r\").data.copy()\n",
    "\n",
    "# Crop out a central (1024, 1024) region of the image\n",
    "data_cropped = data[\n",
    "    data.shape[0] // 2 - 512 : data.shape[0] // 2 + 512,\n",
    "    data.shape[1] // 2 - 512 : data.shape[1] // 2 + 512,\n",
    "]\n",
    "\n",
    "# Save the cropped image to a new MRC file\n",
    "# NOTE: This is not updating any of the header information\n",
    "output_filename = \"mgraphs/xenon_131_000_0.0_DWS_cropped_4.mrc\"\n",
    "with mrcfile.new(output_filename, overwrite=True) as mrc:\n",
    "    mrc.set_data(data_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3119dd",
   "metadata": {},
   "source": [
    "Below, we setup and run a full-orientation match template run based on the downloaded [configuration file](./config/match_template_config_crop.yaml).\n",
    "The programs section of the Leopard-EM documentation contains detailed explanations for each of these fields, so we will continue by running match template.\n",
    "\n",
    "**Note: This config assumes you have 4 GPUs on your system! You may need to change the `gpu_ids` field depending on your system!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ede51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory to save program results\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a97f9",
   "metadata": {},
   "source": [
    "Now we run the match template program (this may take around 1 hour depending on GPU hardware).\n",
    "Alternately, you can skip the next cell and uncomment the code which downloads the already processed results from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/match_template_config_crop.yaml\"\n",
    "ORIENTATION_BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def run_match_template_cropped_image():\n",
    "    \"\"\"Main function to run the match template program on the cropped image.\"\"\"\n",
    "    mt_manager = MatchTemplateManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    mt_manager.run_match_template(ORIENTATION_BATCH_SIZE)\n",
    "    df = mt_manager.results_to_dataframe(locate_peaks_kwargs={\"false_positives\": 1.0})\n",
    "    df.to_csv(\"results/results_match_template_crop.csv\")\n",
    "\n",
    "\n",
    "# NOTE: invoking from `if __name__ == \"__main__\"` is necessary\n",
    "# for proper multiprocessing/GPU-distribution behavior\n",
    "if __name__ == \"__main__\":\n",
    "    run_match_template_cropped_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this to download the cropped match template result files\n",
    "# # fmt: off\n",
    "# file_downloads = [\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/results_match_template_60S.csv\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_correlation_average.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_correlation_variance.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_mip.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_orientation_phi.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_orientation_psi.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_orientation_theta.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_relative_defocus.mrc\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/output_scaled_mip.mrc\"),\n",
    "# ]\n",
    "# # fmt: on\n",
    "\n",
    "# # Loop through files list and download each\n",
    "# for out_dir, file_url in file_downloads:\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # Skip if the file already exists\n",
    "#     fname = file_url.split(\"/\")[-1]\n",
    "#     if os.path.exists(f\"{out_dir}/{fname}\"):\n",
    "#         print(f\"Skipped {fname}, it already exists in {out_dir}.\")\n",
    "#         continue\n",
    "\n",
    "#     # Download the file\n",
    "#     filename = download_zenodo_file(file_url, out_dir)\n",
    "#     print(f\"Downloaded {filename} to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80e868",
   "metadata": {},
   "source": [
    "## 3. Template Optimization\n",
    "\n",
    "As detailed elsewhere, 2DTM is extremely sensitive to pixel size when simulating a reference map.\n",
    "Deposited pixel sizes can be a few percent incorrect, either because the pdb model was built into a map with the wrong pixel size and/or the microscope magnification was calibrated incorrectly.\n",
    "\n",
    "Before proceeding and to maximize our sensitivity, we will run the optimize template program on the cropped image match template run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3cedf",
   "metadata": {},
   "source": [
    "### Running the optimize template program\n",
    "\n",
    "Like the match template program, the optimize template program is configured using a YAML file.\n",
    "We've already downloaded the necessary [yaml file for optimize template](configs/optimize_template_config.yaml); details of the optimize template program and its configuration are detailed elsewhere in the documentation.\n",
    "\n",
    "Alternately, skip this and download the results files directly from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6125d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE_YAML_PATH = \"configs/optimize_template_config.yaml\"\n",
    "\n",
    "\n",
    "def run_optimize_template():\n",
    "    \"\"\"Main function to run the optimize template program.\"\"\"\n",
    "    ot_manager = OptimizeTemplateManager.from_yaml(OPTIMIZE_YAML_PATH)\n",
    "    ot_manager.run_optimize_template(\n",
    "        output_text_path=\"results/optimize_template_results.txt\"\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: invoking from `if __name__ == \"__main__\"` is necessary\n",
    "# for proper multiprocessing/GPU-distribution behavior\n",
    "if __name__ == \"__main__\":\n",
    "    run_optimize_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this to download the optimize template result files\n",
    "# # fmt: off\n",
    "# file_downloads = [\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/optimize_template_results.txt\"),\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/optimize_template_results_all.csv\"),\n",
    "# ]\n",
    "\n",
    "# # Loop through files list and download each\n",
    "# for out_dir, file_url in file_downloads:\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # Skip if the file already exists\n",
    "#     fname = file_url.split(\"/\")[-1]\n",
    "#     if os.path.exists(f\"{out_dir}/{fname}\"):\n",
    "#         print(f\"Skipped {fname}, it already exists in {out_dir}.\")\n",
    "#         continue\n",
    "\n",
    "#     # Download the file\n",
    "#     filename = download_zenodo_file(file_url, out_dir)\n",
    "#     print(f\"Downloaded {filename} to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53a533",
   "metadata": {},
   "source": [
    "By Inspecting the optimize template result file, we see this gave us an optimized pixel size of $0.936$ Angstroms (actual results possibly $\\pm 0.002$ off).\n",
    "We will proceed using this pixel size rather than the original $0.95$ Angstroms value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81779d",
   "metadata": {},
   "source": [
    "### Other parameter considerations\n",
    "\n",
    "The other most obvious and important parameter to optimize is the contrast transfer function B-factor.\n",
    "Since we don't need the most accurate map and highly optimized results for this tutorial, we will skipp this for now and proceed with the default B-factor of $60.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196011c7",
   "metadata": {},
   "source": [
    "## 4. Re-simulating maps of reference templates\n",
    "\n",
    "Now that we know the optimized pixel size of $0.936$ Angstroms, we will re-simulate the maps for both the 40S and 60S templates.\n",
    "Note that the simulation conditions are the same, so we can re-use the simulation configuration object in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_conf = SimulatorConfig(\n",
    "    voltage=300.0,  # in keV\n",
    "    apply_dose_weighting=True,\n",
    "    dose_start=0.0,  # in e-/A^2\n",
    "    dose_end=50.0,  # in e-/A^2\n",
    "    dose_filter_modify_signal=\"rel_diff\",\n",
    "    upsampling=-1,  # auto\n",
    "    mtf_reference=\"falcon4EC_300kv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the simulator for the centered 60S model\n",
    "sim_60S = Simulator(\n",
    "    pdb_filepath=\"models/60S_aligned_aligned_zero.pdb\",\n",
    "    pixel_spacing=0.936,  # Angstroms\n",
    "    volume_shape=(512, 512, 512),\n",
    "    center_atoms=False,\n",
    "    remove_hydrogens=True,\n",
    "    b_factor_scaling=0.5,\n",
    "    additional_b_factor=0,\n",
    "    simulator_config=sim_conf,\n",
    ")\n",
    "\n",
    "# Run the simulation and write the output to a file\n",
    "mrc_filepath_60S = \"maps/60S_map_px0.936_bscale0.5.mrc\"\n",
    "sim_60S.export_to_mrc(mrc_filepath_60S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the simulator for the centered 40S model\n",
    "sim_40S = Simulator(\n",
    "    pdb_filepath=\"models/6q8y_SSU_no_head_aligned_aligned_zero.pdb\",\n",
    "    pixel_spacing=0.936,  # Angstroms\n",
    "    volume_shape=(512, 512, 512),\n",
    "    center_atoms=False,\n",
    "    remove_hydrogens=True,\n",
    "    b_factor_scaling=0.5,\n",
    "    additional_b_factor=0,  # Add to all atoms\n",
    "    simulator_config=sim_conf,\n",
    ")\n",
    "\n",
    "# Run the simulation and write the output to a file\n",
    "mrc_filepath_40S = \"maps/SSU-body_map_px0.936_bscale0.5.mrc\"\n",
    "sim_60S.export_to_mrc(mrc_filepath_40S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb808a1",
   "metadata": {},
   "source": [
    "## 5. Full-image match template\n",
    "\n",
    "It's now time to run the match template program on the 60S ribosome map.\n",
    "The YAML configuration file is similar to the one for the cropped image, but we've now updated the pixel size and referenced the full micrograph.\n",
    "\n",
    "This next cell is a computationally expensive step and takes ~2.75 hours (wall time) on a machine equipped with 4xRTX A6000 ada GPUs.\n",
    "Again, already processed results can be downloaded directly from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/match_template_config_60S.yaml\"\n",
    "ORIENTATION_BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "def run_match_template_60S():\n",
    "    \"\"\"Main function to run the match template program.\"\"\"\n",
    "    mt_manager = MatchTemplateManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    mt_manager.run_match_template(ORIENTATION_BATCH_SIZE)\n",
    "    df = mt_manager.results_to_dataframe(locate_peaks_kwargs={\"false_positives\": 1.0})\n",
    "    df.to_csv(\"results/results_match_template_60S.csv\")\n",
    "\n",
    "\n",
    "# NOTE: invoking from `if __name__ == \"__main__\"` is necessary\n",
    "# for proper multiprocessing/GPU-distribution behavior\n",
    "if __name__ == \"__main__\":\n",
    "    run_match_template_60S()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to download the cropped match template result files\n",
    "# fmt: off\n",
    "file_downloads = [\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/results_match_template_60S.csv\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_correlation_average.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_correlation_variance.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_mip.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_orientation_phi.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_orientation_psi.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_orientation_theta.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_relative_defocus.mrc\"),\n",
    "    (\"results\", \"https://zenodo.org/records/15368246/files/output_scaled_mip.mrc\"),\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "# Loop through files list and download each\n",
    "for out_dir, file_url in file_downloads:\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Skip if the file already exists\n",
    "    fname = file_url.split(\"/\")[-1]\n",
    "    if os.path.exists(f\"{out_dir}/{fname}\"):\n",
    "        print(f\"Skipped {fname}, it already exists in {out_dir}.\")\n",
    "        continue\n",
    "\n",
    "    # Download the file\n",
    "    filename = download_zenodo_file(file_url, out_dir)\n",
    "    print(f\"Downloaded {filename} to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d593a8f",
   "metadata": {},
   "source": [
    "### Looking at results from match template\n",
    "\n",
    "The template matching process has found 408 peaks above the cutoff threshold.\n",
    "However, this micrograph contains the edge of the lamella in the top-right corner, and the dark patch artificially inflates the z-scores in that region (low search variance).\n",
    "We make a scatter plot of the peak locations superimposed on the original micrograph; points are colored by variance over search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image into numpy array called 'data'\n",
    "data = mrcfile.open(\"mgraphs/xenon_131_000_0.0_DWS.mrc\", mode=\"r\").data.copy()\n",
    "\n",
    "# Get the x and y positions of particles from the results csv\n",
    "df = pd.read_csv(\"results/results_match_template_60S.csv\")\n",
    "x = df[\"pos_x_img\"].values\n",
    "y = df[\"pos_y_img\"].values\n",
    "var = df[\"correlation_variance\"].values\n",
    "\n",
    "# Plot the greyscale image\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(data, cmap=\"gray\")\n",
    "plt.scatter(x, y, c=var, cmap=\"bwr\", alpha=0.5)\n",
    "plt.colorbar(label=\"Correlation Variance\")\n",
    "plt.xlabel(\"X Position (pixels)\")\n",
    "plt.ylabel(\"Y Position (pixels)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac11e6",
   "metadata": {},
   "source": [
    "Multiple data processing strategies can help account for these dark patches (or other imaging artifacts) some of which are:\n",
    "\n",
    "* Replacing that region in the image with Gaussian noise with the same mean and standard deviation as the rest of the image,\n",
    "* Excluding peaks picked in the artifact region, and\n",
    "* Filtering based on both the MIP and z-score.\n",
    "\n",
    "For the purposes of this tutorial we simply impose that both MIP and z-score (scaled MIP) are above the cutoff threshold.\n",
    "This will remove these false positives at the cost of increasing the false negative rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57972cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cutoff value\n",
    "# Using the total_correlations from the first row since it's the same for all rows\n",
    "# Also, manually calculating the number of pixels in the result files\n",
    "num_ccg = df[\"total_correlations\"].iloc[0] * (4096 - 512 + 1) ** 2\n",
    "cutoff = gaussian_noise_zscore_cutoff(num_ccg, false_positives=1.0)\n",
    "\n",
    "print(f\"Cutoff value: {cutoff:.4f}\")\n",
    "\n",
    "filtered_df = df[(df[\"mip\"] > cutoff) & (df[\"scaled_mip\"] > cutoff)]\n",
    "filtered_df.to_csv(\"results/results_match_template_60S_edit.csv\")\n",
    "\n",
    "# Print the original and filtered number of peaks\n",
    "print(f\"Original number of peaks: {len(df)}\")\n",
    "print(f\"Filtered number of peaks: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f7401",
   "metadata": {},
   "source": [
    "## 6. Running refine template\n",
    "\n",
    "We will now run the refine template program on the identified 60S particles.\n",
    "Note that this will not find additional LSU particles, but it will improve our estimates fo the location and orientation for each already found particle.\n",
    "The main difference between match template and refine template is match template runs on the entire micrograph over all orientations whereas refine template searches over a particle stack on already oriented particles around local orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b2308",
   "metadata": {},
   "source": [
    "### Inspecting the refine template YAML file\n",
    "\n",
    "We are again using a YAML file to configure our refine template run, and this time we will briefly inspect the YAML file.\n",
    "The `particle_stack` field warrants some discussion.\n",
    "\n",
    "Here, `particle_stack.df_path` is the path to the output csv file from the match template program; this csv file contains per-particle information on location and orientation, and we have already used this csv file to visualize results in the above plot.\n",
    "\n",
    "The two other field, `particle_stack.original_template_size` and `particle_stack.extracted_box_size`, are used to crop out boxes around a particle in the original image.\n",
    "Our map was simulated as a $(512, 512, 512)$ box so the generated projections and therefore original template size field are both $[512, 512]$.\n",
    "The extracted box size here is $[518, 518]$, six pixels (must be even) wider/taller than the projections meaning the peak is constrained to a 6x6 region during the refinement.\n",
    "\n",
    "Again, if your system as fewer/more GPUs, remember to adjust the `gpu_ids` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cecb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(\"configs/refine_template_config_60S.yaml\") as f:\n",
    "    yaml_content = f.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1b7d9",
   "metadata": {},
   "source": [
    "### Running refine template\n",
    "\n",
    "Template refinement is less computationally intensive than full match template, but the results can again be downloaded directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/refine_template_config_60S.yaml\"\n",
    "DATAFRAME_OUTPUT_PATH = \"results/results_refine_template_60S.csv\"\n",
    "PARTICLE_BATCH_SIZE = 64  # Adjust based on your GPU memory\n",
    "\n",
    "\n",
    "def run_refine_template_60S():\n",
    "    \"\"\"Main function to run the refine template program.\"\"\"\n",
    "    rt_manager = RefineTemplateManager.from_yaml(\n",
    "        \"configs/refine_template_config_60S.yaml\"\n",
    "    )\n",
    "\n",
    "    # Ignore UserWarning during refinement call\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        rt_manager.run_refine_template(DATAFRAME_OUTPUT_PATH, PARTICLE_BATCH_SIZE)\n",
    "\n",
    "\n",
    "# NOTE: Invoking program under `if __name__ == \"__main__\"` necessary for multiprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    run_refine_template_60S()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this to download the refine template result files\n",
    "# # fmt: off\n",
    "# file_downloads = [\n",
    "#     (\"results\", \"https://zenodo.org/records/15368246/files/results_refine_template_60S.csv\"),\n",
    "# ]\n",
    "\n",
    "# # Loop through files list and download each\n",
    "# for out_dir, file_url in file_downloads:\n",
    "#     # Create the directory if it doesn't exist\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     # Skip if the file already exists\n",
    "#     fname = file_url.split(\"/\")[-1]\n",
    "#     if os.path.exists(f\"{out_dir}/{fname}\"):\n",
    "#         print(f\"Skipped {fname}, it already exists in {out_dir}.\")\n",
    "#         continue\n",
    "\n",
    "#     # Download the file\n",
    "#     filename = download_zenodo_file(file_url, out_dir)\n",
    "#     print(f\"Downloaded {filename} to {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd320c",
   "metadata": {},
   "source": [
    "## 7. Initial 40S match template search\n",
    "\n",
    "The 40S SSU is smaller and more conformationally flexible than the 60S LSU making it harder to identify using 2DTM.\n",
    "Nonetheless, we will first do a full match template search for the 40S and look at the results.\n",
    "\n",
    "Running an initial full match template search on the constrained particle is also necessary since we need the mean and variance of cross-correlation values over the search space for normalization.\n",
    "Also, we are only searching for the SSU body since it is more rigid leading to higher quality 2DTM results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/match_template_config_40S.yaml\"\n",
    "ORIENTATION_BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "def run_match_template_40S():\n",
    "    \"\"\"Main function to run the match template program.\"\"\"\n",
    "    mt_manager = MatchTemplateManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    mt_manager.run_match_template(ORIENTATION_BATCH_SIZE)\n",
    "    df = mt_manager.results_to_dataframe(locate_peaks_kwargs={\"false_positives\": 1.0})\n",
    "    df.to_csv(\"results/results_match_template_40S.csv\")\n",
    "\n",
    "\n",
    "# NOTE: invoking from `if __name__ == \"__main__\"` is necessary\n",
    "# for proper multiprocessing/GPU-distribution behavior\n",
    "if __name__ == \"__main__\":\n",
    "    run_match_template_40S()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4963d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download cell for pre-processed 40S results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scatter plot of the 40S results like for 60S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da635192",
   "metadata": {},
   "source": [
    "We've found 177 peaks, but many of these fall in the corner of the micrograph in the lamella edge as visualized below.\n",
    "Applying the same filtering cutoff filtering process removes these false-positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8df6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code for filtering the 40S particles and re-plotting their locations along with\n",
    "# 60S particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01e148",
   "metadata": {},
   "source": [
    "Data for both the SSU and LSU have now been processed, and we could attempt to combine these results to classify complete 80S ribosomes based on rotation angle.\n",
    "However, many of the SSU particle are obsucred by noise and therefore irrecoverable.\n",
    "\n",
    "A better strategy would be a constraind search for the 40S particle based on the positions and orientations of already identified 60S particles.\n",
    "By restricting the 2DTM search space (both location and orientation), we lower our noise floor thus increasing our sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98525326",
   "metadata": {},
   "source": [
    "## 8. Constrained Search\n",
    "\n",
    "The 40S subunit has many degrees of freedom. It can rotate relative to the 60S and also within the 40S there is head and body domain that can 'swivel' with respect to one another.\n",
    "For the purpose of this tutorial, we will ignore the latter and consider the 40S as one rigid body moving with respect to the 60S.\n",
    "\n",
    "### ChimeraX processing\n",
    "\n",
    "The first thing we need to do is find out the axis of rotation (between the LSU and SSU) using two PDB models, 3j77 and 3j78, that are in different rotational states.\n",
    "These two models were first processed in ChimeraX using the matchmaker function to align them relative to the 60S model we template matched against.\n",
    "After alignment, only the protein structures in the 40S subunit were selected and exported to the two files: `3j77_SSU_aligned_zero.pdb` and `3j78_SSU_aligned_zero.pdb` which were downloaded previously.\n",
    "\n",
    "### Script for finding the rotation axis\n",
    "\n",
    "We now run the following script, [`Leopard-EM/programs/constrained_search/utils/get_rot_axis.py`](https://github.com/Lucaslab-Berkeley/Leopard-EM/blob/main/programs/constrained_search/utils/get_rot_axis.py), which finds the relative rotation axis between two PDB models and calculates the rotation to align this rotation axis along the \"Z\" direction.\n",
    "The script outputs the Euler angles necessary for alignment which is a necessary input for the constrained search program.\n",
    "\n",
    "In practice, you would run `python get_rot_axis.py <pdb_file1> <pdb_file2> <output_file>`, but to keep the tutorial self-contained, we copy the script functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rotation_axis_angle(\n",
    "    rotmat: torch.Tensor | np.ndarray,\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Extract rotation axis and angle from rotation matrix handling edge cases.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    rotmat: torch.Tensor | np.ndarray\n",
    "        The rotation matrix either as a torch tensor or numpy array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.ndarray, float]\n",
    "        The rotation axis and angle with angle in units of radians.\n",
    "    \"\"\"\n",
    "    rotmat = rotmat.numpy() if isinstance(rotmat, torch.Tensor) else rotmat\n",
    "\n",
    "    rotation = Rotation.from_matrix(rotmat)\n",
    "    rotvec = rotation.as_rotvec()\n",
    "\n",
    "    angle = np.linalg.norm(rotvec)\n",
    "\n",
    "    # Handle edge case for very small angles (near zero)\n",
    "    if np.abs(angle) < 1e-6:\n",
    "        return np.array([0.0, 0.0, 1.0]), angle\n",
    "\n",
    "    # NOTE: Edge case for angles near 180 degrees handled by scipy internally\n",
    "    axis = rotvec / angle\n",
    "\n",
    "    return axis, angle\n",
    "\n",
    "\n",
    "def calculate_axis_euler_angles(axis: torch.Tensor | np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"Calculate Euler angles (ZYZ) that for the rotation axis.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    axis: torch.Tensor | np.ndarray\n",
    "        The rotation axis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float]\n",
    "        The Euler angles in units of degrees\n",
    "    \"\"\"\n",
    "    z_axis = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
    "    axis = axis.numpy() if isinstance(axis, torch.Tensor) else axis\n",
    "\n",
    "    # Edge case for axis already aligned with z-axis\n",
    "    if np.linalg.norm(axis - z_axis) < 1e-6:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    # Edge case for axis anti-aligned with z-axis\n",
    "    if np.linalg.norm(axis + z_axis) < 1e-6:\n",
    "        return 0.0, 180.0\n",
    "\n",
    "    # Calculate theta - angle from z-axis (polar angle)\n",
    "    cos_theta = np.dot(axis, z_axis)\n",
    "    theta = np.acos(np.clip(cos_theta, -1.0, 1.0)) * 180 / np.pi\n",
    "\n",
    "    # Calculate phi - angle in xy plane (azimuthal angle)\n",
    "    phi = np.atan2(axis[1], axis[0]) * 180 / np.pi\n",
    "    if phi < 0:\n",
    "        phi += 360.0  # Convert to 0-360 range\n",
    "\n",
    "    return phi, theta\n",
    "\n",
    "\n",
    "def process_pdb_files(\n",
    "    pdb_file1: str, pdb_file2: str\n",
    ") -> tuple[np.ndarray, float, float, float]:\n",
    "    \"\"\"Helper function to calculate the rotation axis and angle for two PDB files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_file1: str\n",
    "        Path to the first PDB file.\n",
    "    pdb_file2: str\n",
    "        Path to the second PDB file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[np.ndarray, float, float, float]\n",
    "        The rotation axis, rotation angle in radians, and Euler angles (phi, theta).\n",
    "    \"\"\"\n",
    "    # Read PDB files\n",
    "    df1 = mmdf.read(pdb_file1)\n",
    "    df2 = mmdf.read(pdb_file2)\n",
    "\n",
    "    # Extract coordinates\n",
    "    coords1 = torch.tensor(df1[[\"x\", \"y\", \"z\"]].values, dtype=torch.float32)\n",
    "    coords2 = torch.tensor(df2[[\"x\", \"y\", \"z\"]].values, dtype=torch.float32)\n",
    "\n",
    "    # Center coordinates\n",
    "    centroid1 = coords1.mean(dim=0)\n",
    "    centroid2 = coords2.mean(dim=0)\n",
    "    coords1_centered = coords1 - centroid1\n",
    "    coords2_centered = coords2 - centroid2\n",
    "\n",
    "    # Calculate rotation matrix\n",
    "    rotation_matrix, _ = roma.rigid_points_registration(\n",
    "        coords1_centered, coords2_centered\n",
    "    )\n",
    "\n",
    "    # Extract rotation axis and angle plus Euler angles\n",
    "    rotation_axis, rotation_angle = extract_rotation_axis_angle(rotation_matrix)\n",
    "    phi, theta = calculate_axis_euler_angles(rotation_axis)\n",
    "\n",
    "    # radians to degrees\n",
    "    rotation_angle = np.rad2deg(rotation_angle)\n",
    "\n",
    "    return rotation_axis, rotation_angle, phi, theta\n",
    "\n",
    "\n",
    "def write_results(\n",
    "    output_file: str,\n",
    "    pdb_file1: str,\n",
    "    pdb_file2: str,\n",
    "    rotation_axis: np.ndarray,\n",
    "    rotation_angle: float,\n",
    "    phi: float,\n",
    "    theta: float,\n",
    ") -> None:\n",
    "    \"\"\"Helper function to write the script results to a file.\"\"\"\n",
    "    suggested_range = min(30.0, max(10.0, rotation_angle / 2))\n",
    "    results_string = f\"\"\"# PDB Rotation Analysis Results\\n\n",
    "    Source PDB: {pdb_file1}\n",
    "    Target PDB: {pdb_file2}\n",
    "\n",
    "    ## Rotation Parameters\n",
    "    Axis: {rotation_axis[0]:.6f} {rotation_axis[1]:.6f} {rotation_axis[2]:.6f}\n",
    "    Angle: {rotation_angle:.6f} degrees\\n\n",
    "\n",
    "    ## Axis Orientation Angles (input for constrained search config)\n",
    "    rotation_axis_euler_angles: [{phi:.2f}, {theta:.2f}, 0.0]\\n\n",
    "\n",
    "    ## Example constrained search config\n",
    "    orientation_refinement_config:\n",
    "      enabled: true\n",
    "      out_of_plane_step: 1.0   # Step size around the rotation axis\n",
    "      in_plane_step: 0.5       # Step size for fine adjustment angles\n",
    "      rotation_axis_euler_angles: [{phi:.2f}, {theta:.2f}, 0.0]\n",
    "      phi_min: -{suggested_range:.1f}  # Search range for around the axis\n",
    "      phi_max: {suggested_range:.1f}\n",
    "      theta_min: -2.0  # Small adjustments perpendicular to axis (optional)\n",
    "      theta_max: 2.0\n",
    "      psi_min: -2.0    # Small in-plane adjustments (optional)\n",
    "      psi_max: 2.0\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the script results to the console\n",
    "    print(results_string)\n",
    "\n",
    "    # And also write them to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(results_string)\n",
    "\n",
    "    print(f\"Rotation analysis written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6768cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_file1 = \"models/3j77_SSU_aligned_zero.pdb\"\n",
    "pdb_file2 = \"models/3j78_SSU_aligned_zero.pdb\"\n",
    "output_file = \"results/rotation_axis.txt\"\n",
    "\n",
    "rotation_axis, rotation_angle, phi, theta = process_pdb_files(pdb_file1, pdb_file2)\n",
    "write_results(\n",
    "    output_file,\n",
    "    pdb_file1,\n",
    "    pdb_file2,\n",
    "    rotation_axis,\n",
    "    rotation_angle,\n",
    "    phi,\n",
    "    theta,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba94c28",
   "metadata": {},
   "source": [
    "These Euler angles tell us the rotation matrix needed to align rotations along the Z-axis meaning we can perform a simple angular search over the phi/psi range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a018c",
   "metadata": {},
   "source": [
    "### Determining relative defocus of constrained particle\n",
    "\n",
    "As well as constraining orientations and $(x, y)$ positions, we can eliminate the defocus search.\n",
    "The constrained search program uses the defocus and orientation of the 60S particle to work out the defocus of the 40S particle.\n",
    "However, we need to provide the offset between the two subunits which can be calculated using this script: [`Leopard-EM/programs/constrained_search/utils/get_center_vector.py`](https://github.com/Lucaslab-Berkeley/Leopard-EM/blob/main/programs/constrained_search/utils/get_center_vector.py).\n",
    "The contents of the script are copied below to keep the tutorial self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_position(df: pd.DataFrame) -> torch.Tensor:\n",
    "    \"\"\"Calculate the mean position of a PDB structure loaded into a dataframe.\"\"\"\n",
    "    coords = torch.tensor(df[[\"x\", \"y\", \"z\"]].values, dtype=torch.float32)\n",
    "    mean_pos = coords.mean(dim=0)\n",
    "\n",
    "    return mean_pos\n",
    "\n",
    "\n",
    "def calculate_relative_vectors(pdb_file1: str, pdb_file2: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the relative position and orientation vectors between two PDB structures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_file1 : str\n",
    "        Path to the first PDB file\n",
    "    pdb_file2 : str\n",
    "        Path to the second PDB file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing relative vector data including:\n",
    "        - df1, df2: DataFrames for both PDB files\n",
    "        - vector: Vector from PDB1 to PDB2\n",
    "        - euler_angles: Phi, Theta, Psi angles\n",
    "        - z_diff: Z height difference\n",
    "        - defocus_description: Human-readable defocus description\n",
    "    \"\"\"\n",
    "    # Parse PDB files using mmdf\n",
    "    df1 = mmdf.read(pdb_file1)\n",
    "    df2 = mmdf.read(pdb_file2)\n",
    "\n",
    "    print(f\"File 1: {pdb_file1} - {len(df1)} atoms\")\n",
    "    print(f\"File 2: {pdb_file2} - {len(df2)} atoms\")\n",
    "\n",
    "    # Calculate mean positions at default orientation (0, 0, 0)\n",
    "    mean_pos1 = calculate_mean_position(df1)\n",
    "    mean_pos2 = calculate_mean_position(df2)\n",
    "\n",
    "    # Calculate vector from PDB1 to PDB2\n",
    "    vector = mean_pos2 - mean_pos1\n",
    "\n",
    "    # Convert vector to Euler angles\n",
    "    phi, theta, psi = roma.rotvec_to_euler(\n",
    "        convention=\"ZYZ\", rotvec=vector, degrees=True, as_tuple=True\n",
    "    )\n",
    "\n",
    "    # Calculate Z-height difference (defocus)\n",
    "    z_diff = vector[2].item()\n",
    "    defocus_description = (\n",
    "        f\"{abs(z_diff):.2f} Angstroms {'below' if z_diff < 0 else 'above'}\"\n",
    "    )\n",
    "\n",
    "    # Print initial results\n",
    "    initial_results = f\"\"\"Initial Analysis:\n",
    "    Vector from PDB1 to PDB2: [{vector[0]:.6f}, {vector[1]:.6f}, {vector[2]:.6f}]\n",
    "    Vector Euler angles (ZYZ, deg): Phi={phi:.2f}, Theta={theta:.2f}, Psi={psi:.2f}\n",
    "    Z-height difference (defocus): {defocus_description}\n",
    "    \"\"\"\n",
    "    print(initial_results)\n",
    "\n",
    "    return {\n",
    "        \"df1\": df1,\n",
    "        \"df2\": df2,\n",
    "        \"vector\": vector,\n",
    "        \"euler_angles\": (phi, theta, psi),\n",
    "        \"z_diff\": z_diff,\n",
    "        \"defocus_description\": defocus_description,\n",
    "    }\n",
    "\n",
    "\n",
    "def process_rotations(vector: torch.Tensor, num_rotations: int) -> list:\n",
    "    \"\"\"\n",
    "    Process each rotation and calculate the resulting defocus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector : torch.Tensor\n",
    "        The original vector between structures\n",
    "    num_rotations : int\n",
    "        Number of random rotations to test (in addition to default orientation)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of dictionaries with defocus results for each rotation\n",
    "    \"\"\"\n",
    "    print(\"\\nDefocus changes for different rotations:\")\n",
    "    defocus_results = []\n",
    "\n",
    "    for i in range(num_rotations + 1):\n",
    "        if i == 0:\n",
    "            rand_rotmat = torch.eye(3)\n",
    "        else:\n",
    "            rand_rotmat = roma.random_rotmat()\n",
    "\n",
    "        rand_euler = roma.rotmat_to_euler(\"ZYZ\", rand_rotmat, degrees=True)\n",
    "        rotated_vector = rand_rotmat @ vector\n",
    "\n",
    "        # Extract new z-component (defocus)\n",
    "        new_z_diff = rotated_vector[2].item()\n",
    "        new_defocus = (\n",
    "            f\"{abs(new_z_diff):.2f} Angstroms {'below' if new_z_diff < 0 else 'above'}\"\n",
    "        )\n",
    "        print(f\"Rotation #{i+1} - {rand_euler}: Defocus = {new_defocus}\")\n",
    "\n",
    "        defocus_results.append(\n",
    "            {\n",
    "                \"rotation\": i + 1,\n",
    "                \"euler_angles\": [angle.item() for angle in rand_euler],\n",
    "                \"defocus\": new_z_diff,\n",
    "                \"description\": new_defocus,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return defocus_results\n",
    "\n",
    "\n",
    "def write_results_to_file(\n",
    "    output_file: str,\n",
    "    pdb_file1: str,\n",
    "    pdb_file2: str,\n",
    "    vector_info: dict,\n",
    "    defocus_results: list,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write analysis results to output file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_file : str\n",
    "        Path to output file\n",
    "    pdb_file1 : str\n",
    "        Path to first PDB file\n",
    "    pdb_file2 : str\n",
    "        Path to second PDB file\n",
    "    vector_info : dict\n",
    "        Dictionary with vector data from calculate_relative_vectors\n",
    "    defocus_results : list\n",
    "        List of defocus results from process_rotations\n",
    "    \"\"\"\n",
    "    vector = vector_info[\"vector\"]\n",
    "    phi, theta, psi = vector_info[\"euler_angles\"]\n",
    "    defocus_description = vector_info[\"defocus_description\"]\n",
    "\n",
    "    result_string = f\"\"\"# PDB Vector and Defocus Analysis\n",
    "    Source PDB 1: {pdb_file1}\n",
    "    Source PDB 2: {pdb_file2}\n",
    "\n",
    "    ## Initial Vector Analysis\n",
    "    Vector PDB1-PDB2: [{vector[0]:.6f}, {vector[1]:.6f}, {vector[2]:.6f}]\n",
    "    Vector Eulers (ZYZ, deg): Phi={phi:.2f}, Theta={theta:.2f}, Psi={psi:.2f}\n",
    "    Z-height difference (defocus): {defocus_description}\n",
    "\n",
    "    ## Defocus changes for different rotations\n",
    "    \"\"\"\n",
    "    for result in defocus_results:\n",
    "        euler = result[\"euler_angles\"]\n",
    "        result_string += (\n",
    "            f\"    Rotation #{result['rotation']} - \"\n",
    "            f\"    Euler({euler[0]:.2f}, {euler[1]:.2f}, {euler[2]:.2f}): \"\n",
    "        )\n",
    "        result_string += f\"Defocus = {result['description']}\\n\"\n",
    "\n",
    "    # Write results to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(result_string)\n",
    "\n",
    "    print(f\"\\nAnalysis results written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"results/rotation_defocus_analysis.txt\"\n",
    "pdb_file1 = \"models/60S_aligned_aligned_zero.pdb\"\n",
    "pdb_file2 = \"models/6q8y_SSU_no_head_aligned_aligned_zero.pdb\"\n",
    "\n",
    "vector_info = calculate_relative_vectors(pdb_file1, pdb_file2)\n",
    "defocus_results = process_rotations(vector_info[\"vector\"], 5)\n",
    "write_results_to_file(output_file, pdb_file1, pdb_file2, vector_info, defocus_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089947ba",
   "metadata": {},
   "source": [
    "The piece of information we need for the constrained search is the relative vector from the first PDB (reference 60S model) to the second PDB (40S without head).\n",
    "In this case, the positional vector is `[88.023109, 52.080257, 45.528008]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ad206",
   "metadata": {},
   "source": [
    "### Configuring and running the constrained search\n",
    "\n",
    "Now that the hard work of obtaining the necessary info for configuring the constrained search, we can move onto actually running the program.\n",
    "The constrained search is a balance between limiting the number of cross-correlations to minimize noise and increasing the 2DTM SNR with progressively finer sampling.\n",
    "We employ a multi-step approach in this tutorial to strike this balance.\n",
    "\n",
    "Just like the other programs in Leopard-EM, we have a YAML configuration file for the constrained search program.\n",
    "Our first search is around the Z-axis done by specifying a range for angle psi.\n",
    "\n",
    "*Note: The Euler angles phi and psi, in this case, are degenerate, and we could equivalently proceed using phi in-place of psi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98001351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(\"configs/constrained_config_step1.yaml\") as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "# Display as markdown code block\n",
    "display(Markdown(f\"```yaml\\n{yaml_content}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/constrained_config_step1.yaml\"\n",
    "DATAFRAME_OUTPUT_PATH = \"results/constrained_search_results_step1.csv\"\n",
    "PARTICLE_BATCH_SIZE = 64\n",
    "FALSE_POSITIVES = 0.005  # False positives per particle\n",
    "\n",
    "\n",
    "def run_constrained_search_step1():\n",
    "    \"\"\"Main function to run the constrained search program.\"\"\"\n",
    "    cs_manager = ConstrainedSearchManager.from_yaml(YAML_CONFIG_PATH)\n",
    "\n",
    "    # Ignore UserWarning during refinement call\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        cs_manager.run_constrained_search(\n",
    "            DATAFRAME_OUTPUT_PATH,\n",
    "            locate_peaks_kwargs={\"false_positives\": FALSE_POSITIVES},\n",
    "            particle_batch_size=PARTICLE_BATCH_SIZE,\n",
    "        )\n",
    "    cs_manager = ConstrainedSearchManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    cs_manager.run_constrained_search(\n",
    "        output_dataframe_path=DATAFRAME_OUTPUT_PATH,\n",
    "        false_positives=FALSE_POSITIVES,\n",
    "        orientation_batch_size=PARTICLE_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: Invoking program under `if __name__ == \"__main__\"` necessary for multiprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    run_constrained_search_step1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7587a",
   "metadata": {},
   "source": [
    "We have increased the number of 40S picks by over an order of magnitude, from 16 to 262, using the constrained 2DTM search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_40S_full = pd.read_csv(\n",
    "    \"results/results_match_template_40S_filtered.csv\"\n",
    ")  # TODO: find appropriate name\n",
    "df_40S_constrained_step1 = pd.read_csv(\"results/constrained_search_results_step1.csv\")\n",
    "\n",
    "print(f\"Full match template 40S particles: {len(df_40S_full)}\")\n",
    "print(f\"Constrained match template search: {len(df_40S_constrained_step1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba43e8",
   "metadata": {},
   "source": [
    "### Running the second constrained search\n",
    "\n",
    "There could be a further benefit to searching over an axis orthogonal to the Z-axis.\n",
    "We will do this in two steps, where a search over each rotation axis is considered independently, to minimize the total number of cross-correlations calculated and control the noise floor.\n",
    "\n",
    "We can define the direction of this orthogonal axis using the `roll_axis` parameter (default is Y-axis with `[0, 1]`).\n",
    "If the rotation in the orthogonal direction is significant, then we could find the best roll axis to search over by setting the `search_roll_axis` field to true.\n",
    "However, we expect the rotation to be small, se we just use the default axis for now.\n",
    "\n",
    "The file [`configs/constrained_config_step2.yaml`](./configs/constrained_config_step2.yaml) contains the updated orientation search parameters for the roll axis, and we perform a second constrained search using the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc273d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/constrained_config_step2.yaml\"\n",
    "DATAFRAME_OUTPUT_PATH = \"results/constrained_search_results_step2.csv\"\n",
    "PARTICLE_BATCH_SIZE = 64\n",
    "FALSE_POSITIVES = 0.005  # False positives per particle\n",
    "\n",
    "\n",
    "def run_constrained_search_step2():\n",
    "    \"\"\"Main function to run the constrained search program.\"\"\"\n",
    "    cs_manager = ConstrainedSearchManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    cs_manager.run_constrained_search(\n",
    "        output_dataframe_path=DATAFRAME_OUTPUT_PATH,\n",
    "        false_positives=FALSE_POSITIVES,\n",
    "        orientation_batch_size=PARTICLE_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: Invoking program under `if __name__ == \"__main__\"` necessary for multiprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    run_constrained_search_step2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0790653",
   "metadata": {},
   "source": [
    "### Running the third & fourth constrained searches\n",
    "\n",
    "The 40S parameters can be further refined, and we could potentially identify more particles.\n",
    "But at each iteration, we accumulate more cross-correlations which in the 2DTM noise model will raise our noise floor.\n",
    "Again, this is a balance of maximizing sensitivity while controlling for noise.\n",
    "\n",
    "We will perform two more successive constrained searches, each with a finer angular sampling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d395824",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/constrained_config_step3.yaml\"\n",
    "DATAFRAME_OUTPUT_PATH = \"results/constrained_search_results_step3.csv\"\n",
    "PARTICLE_BATCH_SIZE = 64\n",
    "FALSE_POSITIVES = 0.005  # False positives per particle\n",
    "\n",
    "\n",
    "def run_constrained_search_step3():\n",
    "    \"\"\"Main function to run the constrained search program.\"\"\"\n",
    "    cs_manager = ConstrainedSearchManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    cs_manager.run_constrained_search(\n",
    "        output_dataframe_path=DATAFRAME_OUTPUT_PATH,\n",
    "        false_positives=FALSE_POSITIVES,\n",
    "        orientation_batch_size=PARTICLE_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: Invoking program under `if __name__ == \"__main__\"` necessary for multiprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    run_constrained_search_step3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d707ed44",
   "metadata": {},
   "source": [
    "During the fourth and final constrained search, we also include a defocus search (configured within the step 4 YAML file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG_PATH = \"configs/constrained_config_step4.yaml\"\n",
    "DATAFRAME_OUTPUT_PATH = \"results/constrained_search_results_step4.csv\"\n",
    "PARTICLE_BATCH_SIZE = 64\n",
    "FALSE_POSITIVES = 0.005  # False positives per particle\n",
    "\n",
    "\n",
    "def run_constrained_search_step4():\n",
    "    \"\"\"Main function to run the constrained search program.\"\"\"\n",
    "    cs_manager = ConstrainedSearchManager.from_yaml(YAML_CONFIG_PATH)\n",
    "    cs_manager.run_constrained_search(\n",
    "        output_dataframe_path=DATAFRAME_OUTPUT_PATH,\n",
    "        false_positives=FALSE_POSITIVES,\n",
    "        orientation_batch_size=PARTICLE_BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: Invoking program under `if __name__ == \"__main__\"` necessary for multiprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    run_constrained_search_step4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3ba38",
   "metadata": {},
   "source": [
    "## 9. Bringing all the results together\n",
    "\n",
    "We now need to combine the results from the four steps in our constrained search together.\n",
    "There is a helper script [`Leopard-EM/programs/constrained_search/sequential_threshold_processing.py`](TODO-link) which does this automatically (assuming the file name formatting follows this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd610a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include code for the sequential thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475074e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2dtm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
